[model]
@architectures = "TransformerDecoder"
n_blocks = 32
block_size = 4096
prenorm = True

[model.embedding]
@layers = "TokenEmbedding"
n_embeddings = 55296
embedding_size = 4096

[model.attention]
@layers = "CausalSelfAttention"
n_in = 4096
n_heads = 32
q_bias = false
k_bias = false
v_bias = false
o_bias = false

[model.feedforward]
@layers = "SwiGLU"
n_in = 4096
n_hidden = 11008

[model.head]
@layers = "LMHead"
n_in = 4096
n_out = 55296
bias = false

[model.norm]
@layers = "RMSNorm"
n_in = 4096
eps = 1e-5