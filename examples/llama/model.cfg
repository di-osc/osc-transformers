[model]
@layers = decoder.roformer
n_blocks = 32
block_size = 512

[model.token_embeddings]
@layers = token_embeddings 
n_tokens = 32000
n_embd = 768

[model.block]
@layers = decoder_block.prenorm

[model.block.attention]
@layers = causal_attention.rope
dim = 768
n_heads = 12

[model.block.attention_norm]
@layers = rmsnorm
n_in = 768

[model.block.mlp]
@layers = mlp.swiglu
size = 768
intermediate_size = 3072

[model.block.mlp_norm]
@layers = rmsnorm
size = 768

[model.head]
@layers = head.one
embedding_size = 768
vocab_size = 32000
