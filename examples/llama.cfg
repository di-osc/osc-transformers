[model]
@architectures = "Decoder.v1"
n_token_embeddings = 32000
embedding_size = 1024
n_blocks = 32
block_size = 4096
pre_norm = True

[model.attention]
@layers = "SelfAttention.v1"
n_in = 1024
n_heads = 32

[model.mlp]
@layers = "SwiGLU"
n_in = 1024
n_hidden = 3072

[model.norm]
@layers = "RMSNorm"
n_in = 1024
eps = 1e-5